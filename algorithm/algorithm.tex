\documentclass[UTF8]{ctexart}
\usepackage{geometry}
\geometry{papersize={21cm,29.7cm}}
\geometry{left=1cm,right=1cm,top=2cm,bottom=2cm}
\usepackage{indentfirst}
\setlength{\parindent}{2.45em}
\usepackage{setspace}
\onehalfspacing
\addtolength{\parskip}{-0.4em}
\usepackage{amsmath}
\title{算法面试}
\author{方鑫}
\date{\today}
\begin{document}
\maketitle
\tableofcontents
\section{数学基础}
\subsection{SGD,Momentum,Adagard,Adam}
\paragraph{SGD：随机梯度下降，每一次迭代计算数据集的一个数据就更新梯度，然后对参数进行更新。}
\paragraph{Momentum：参考物理中动量的概念，前几次的梯度也会参与到当前的计算中，但前几轮的梯度叠加在当前计算中会有一定的衰减。}
\paragraph{Adagrad：在训练过程中可以自动变更学习的速率，设置一个全局的学习率，而实际的学习率与以往的参数模和的平方成反比。}
\paragraph{Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率，在经过偏置的校正后，每一次迭代后的学习率都有个确定的范围，使得参数较为平稳。}

\subsection{PCA原理}
\paragraph{PCA是一种线性降维方法，通过将高维数据映射到低维数据中，使得在新的投影上自身方差尽量大，方差越大特征越有特效，尽量使产生的新特征间相关性越小}
\paragraph{PCA算法具体操作为对所有样本进行中心化操作，计算样本的协方差矩阵，然后对协方差矩阵做特征值分解，取最大的n个特征值对应的特征向量构成投影矩阵}

\subsection{L1不可导如何处理}

\subsection{sigmoid函数特性}
\paragraph{函数形式$f\left(x\right)=\frac{1}{1+{e}^{-x}}$,定义域$\left(-\infty,+\infty\right)$,值域$\left(0,1\right)$}


\section{图像处理算法}
\subsection{图像中的锐化平滑操作}
\subsection{sift特征提取和匹配的具体步骤}
\paragraph{检测流程：对图像用不同的卷积核做卷积，得到不同的卷积结果（即不同的尺度空间），然后对得到的结果做差分。缩小图像比例，重复这一过程得到高斯差分金字塔。对金字塔每个点的邻域，搜索26邻域（三维），找到候选极值。}
\paragraph{生成高斯差分金字塔}
\paragraph{尺度空间构建}
\paragraph{空间极值点检测}
\paragraph{关键点精确定位}
\paragraph{关键点主方向计算}
\paragraph{描述子构造}

\section{机器学习算法}
\subsection{交叉熵}
\paragraph{}
\subsection{训练集中类别不平衡怎么处理？}
\paragraph{类别不均衡指分类任务中不同类别的训练样例数目差别很大的情况。}
\paragraph{类别不均衡的一个基本策略：再缩放。}
\paragraph{正常情况下，如下情况下预测为正}
\begin{equation}
    \frac{y}{1-y}>1
\end{equation}
\paragraph{若正反比例不相同时，为了平衡正负样本，预测几率需要跟随观测几率做出改变}
\begin{equation}
    \frac{y}{1-y}>\frac{{m}^{+}}{{m}^{-}}
\end{equation}
\paragraph{最终在预测时，根据下式得到结果}
\begin{equation}
    \frac{y^{'}}{1-y^{'}}=\frac{y}{1-y}*\frac{{m}^{+}}{{m}^{-}}
\end{equation}
\paragraph{实际操作中，再缩放很困难，因为训练集是真实样本总体的无偏采样这个假设往往不成立。若反例比正例多，实际操作总体有三类做法：1）直接对训练集反类样例欠采样；2）对正类样例过采样；3）直接基于原始训练集进行学习，但在预测时乘上修正比例}
\section{深度学习}
\end{document}